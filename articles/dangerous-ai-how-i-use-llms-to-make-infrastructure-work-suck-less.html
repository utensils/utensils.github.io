<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png" data-next-head=""/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png" data-next-head=""/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png" data-next-head=""/><link rel="manifest" href="/favicon/site.webmanifest" data-next-head=""/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000" data-next-head=""/><link rel="shortcut icon" href="/favicon/favicon.ico" data-next-head=""/><meta name="msapplication-TileColor" content="#000000" data-next-head=""/><meta name="msapplication-config" content="/favicon/browserconfig.xml" data-next-head=""/><meta name="theme-color" content="#000" data-next-head=""/><link rel="alternate" type="application/rss+xml" href="/feed.xml" data-next-head=""/><meta name="description" content="Home of the premier product studio" data-next-head=""/><title data-next-head="">Forking Around: Dangerous AI - How I Use LLMs to Make Infrastructure Work Suck Less | Utensils.io</title><meta property="og:title" content="Forking Around: Dangerous AI - How I Use LLMs to Make Infrastructure Work Suck Less | Utensils.io" data-next-head=""/><meta property="og:description" content="A practical look at how AI can transform infrastructure management, debug complex issues, and save your ADHD brain from the tedium of log analysis." data-next-head=""/><meta property="og:image" content="https://utensils.io/images/articles/dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less/social-share.jpg" data-next-head=""/><meta property="og:url" content="https://utensils.io/articles/dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less" data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:site_name" content="Utensils.io" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:title" content="Forking Around: Dangerous AI - How I Use LLMs to Make Infrastructure Work Suck Less | Utensils.io" data-next-head=""/><meta name="twitter:description" content="A practical look at how AI can transform infrastructure management, debug complex issues, and save your ADHD brain from the tedium of log analysis." data-next-head=""/><meta name="twitter:image" content="https://utensils.io/images/articles/dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less/social-share.jpg" data-next-head=""/><link rel="preload" href="/_next/static/css/612991ceded497d9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/612991ceded497d9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/adcee6828f80b03e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/adcee6828f80b03e.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8cac0b4b405cede1.js" defer=""></script><script src="/_next/static/chunks/framework-ee17a4c43a44d3e2.js" defer=""></script><script src="/_next/static/chunks/main-490c8776b93da852.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0f6671f34fd978fc.js" defer=""></script><script src="/_next/static/chunks/695-4568df259c13997f.js" defer=""></script><script src="/_next/static/chunks/568-37e5a513d0fa129c.js" defer=""></script><script src="/_next/static/chunks/pages/articles/%5Bslug%5D-eea6458013ccb4f1.js" defer=""></script><script src="/_next/static/Hw8_6jlXD1DGirDn_uXZH/_buildManifest.js" defer=""></script><script src="/_next/static/Hw8_6jlXD1DGirDn_uXZH/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen"><main><div class="container mx-auto px-5"><section class="flex-col flex items-left mt-20"><div class="flex-row flex items-left text-left"><a class="flex title-font font-medium items-center md:justify-start justify-center mr-4" href="/"><img src="/logo.png" class="w-12 h-12 mb-2"/></a><a href="/articles" class="pt-1 text-3xl text-red-500 hover:text-red-600 font-black hover:underline mr-4">Articles</a><a href="https://github.com/utensils" target="_blank" class="pt-1 text-3xl text-red-500 hover:text-red-600 font-black hover:underline">GitHub</a></div><div class="flex-col flex items-left mt-10"><h1 class="text-8xl font-black tracking-tighter leading-tight md:pr-8">Utensils</h1><span class="lowercase text-4xl text-gray-400">Forking Around: Dangerous AI - How I Use LLMs to Make Infrastructure Work Suck Less</span></div></section><article><div class="max-w-2xl mx-right pl-6 pt-6"><div class="text-lg font-bold"><time dateTime="2025-04-01">April 1, 2025</time></div><div class="flex flex-inline pb-2 pt-4"><div class="flex items-center"><div class="text-2xl font-bold text-red-500 hover:text-red-600 pr-2"><a href="https://github.com/jamesbrink" target="_blank" rel="noopener noreferrer">James Brink</a></div></div></div></div><div class="max-w-2xl mx-right pl-6 prose"><span class="text-gray-400 text-bold text-xl">A practical look at how AI can transform infrastructure management, debug complex issues, and save your ADHD brain from the tedium of log analysis.</span><div class="markdown-styles_markdown__HwUWZ"><h1>Forking Around: Dangerous AI</h1>
<h2>How I Use LLMs to Make Infrastructure Work Suck Less</h2>
<p><em>By James Brink, Tinkerer of Terror</em></p>
<p>Welcome to another installment of "Forking Around" - where I document my chaotic workflows as I stumble through the brave new world of AI. I'm shocked I actually wrote a second post, defying all expectations.</p>
<h2>Beyond Vibe Coding: AI for the Infrastructure Crowd</h2>
<p>I've been seeing a flood of blogs and videos lately about AI coding, specifically this new "vibe coding" trend. But I'm not seeing much about infrastructure, sysadmin, or operations use cases being shared. So I figured I'd show just one of many ways I dangerously use AI to make my work much faster.</p>
<h2>The Great Ubuntu â†’ NixOS Migration Adventure</h2>
<p>I recently migrated a production server from Ubuntu to NixOS. I had done the original server setup well over a year ago, and since I'm an infra-as-code fanatic, I naturally used Ansible. Over time I slowly learned about NixOS and became more comfortable with it in production. When we needed to take the server offline to do some storage-related work, I decided this was a prime opportunity to ditch my miles of messy Ansible and sneak in NixOS.</p>
<p>I branched, tested locally for a few days with a VM verifying everything was in order, and after running final backups on the prod box, I used my shiny new NixOS configuration and nixos-anywhere to install NixOS remotely over SSH, completely replacing Ubuntu. This worked flawlessly.</p>
<p>So I killed a few birds with one stone and was very happy... things were working great, then one day our app randomly became unavailable. Simple Docker app behind an Nginx proxy... I quickly restarted it and all was well, but I introduced a new problem, and this really is not shocking with such a big migration. At the time of the incident, I did not have time to dig into why it happened, but I did have a gut feeling about the issue. For more context, the container was still live, no errors, the Nginx proxy was running but reporting timeouts... same old story.</p>
<h2>Debugging with ADHD Brain: The Old Way vs. The AI Way</h2>
<p>When I finally had time a few days later to dig into the issue, I had to mentally prepare myself for digging through all the logs, trying to keep the time offsets correct in my ADHD brain (I will never be able to do simple math in my head ðŸ˜‚).</p>
<p>I said fuck this... you know what... opened up my NixOS repo for this server in Windsurf, from here I just did the following:</p>
<p>Yo, read our project structure, specifically around Podman, and Nginx services and the specific Podman service/container that failed.</p>
<p>Then I scrolled back through the Slack channel to find the incident... copy-pasta'd that into the prompt, with a note specifying my time zone vs. the server's so it's aware of the offsets.</p>
<p>I told the agent DO NOT CHANGE ANYTHING, and told it to ssh user@server -c  to run through all the logs and investigate the issue for me and report back its findings.</p>
<h2>The Forensic Magic</h2>
<p>It found the log entries, and when I followed up with the fact the server was recently migrated and gave it the original docker-compose, it was able to confirm with confidence that the issue was truly just cgroup related. I did not explicitly set up Podman with enough resources, and this was further confirmed by the non-error messages about Puma's current memory usage.</p>
<p>Here's a snippet of the timeline that the AI put together (because it's a cold-blooded machine that doesn't feel my ADHD pain):</p>
<ul>
<li>16:54:56 EDT: First "upstream timed out" error in nginx logs</li>
<li>16:56:03 EDT: Second "upstream timed out" error ( records API)</li>
<li>16:57:26 EDT: Third "upstream timed out" error</li>
<li>17:37:00 EDT: Issues first reported in chat</li>
<li>17:41:21 - 17:42:56 EDT: PumaWorkerKiller consistently reporting high memory usage (656.6MB)</li>
<li>17:48:24 EDT: Systemd automatically restarted the service</li>
<li>17:49:39 EDT: Service fully restored</li>
</ul>
<p>Look at that clean timeline! Do you have any idea how long it would have taken my scattered brain to piece that together manually? And it did the boring work of correlating the exact failure patterns across multiple log sources.</p>
<p>Now none of the above is terribly groundbreakingâ€”any sysadmin could hunt these kinds of issues down. The real difference is the absolute speed at which I was able to find the root cause. This only took a few minutes at most.</p>
<p>Once I was satisfied and in agreement with the agent, I asked it to create a postmortem report for me and include the timeline of events. What it came back with was incredibly comprehensive, including a detailed root cause analysis that identified these issues:</p>
<h2>Root Cause Analysis</h2>
<p>The root cause of the service disruption was a combination of factors related to the recent migration from Ubuntu/Docker to NixOS/Podman:</p>
<ol>
<li>
<p>Resource Management Differences:</p>
<ul>
<li>The Rails application was developed and tested in a Docker environment without explicit resource limits</li>
<li>When migrated to Podman on NixOS, the application encountered different resource management behavior due to cgroup v2</li>
<li>PumaWorkerKiller logs showed consistent high memory usage (657MB) before the failure</li>
</ul>
</li>
<li>
<p>Container Runtime Differences:</p>
<ul>
<li>The migration from Docker to Podman introduced subtle differences in how containers are managed</li>
<li>The cgroup configuration in NixOS wasn't properly set up to accommodate the container's resource needs</li>
</ul>
</li>
</ol>
<p>It even compared the original Docker config to my new NixOS setup:</p>
<h2>Configuration Comparison</h2>
<p>Docker Configuration:</p>
<pre><code>web:
  image: &#x3C;REDACTED>
  restart: always
  network_mode: host
  environment:
    - VIRTUAL_PROTO=http
    - VIRTUAL_PORT=20420
    - VIRTUAL_HOST=&#x3C;REDACTED>
</code></pre>
<p>NixOS/Podman Configuration:</p>
<pre><code>web = {
  image = "&#x3C;REDACTED>";
  autoStart = true;
  extraOptions = [ "--network=host" ];
  environment = {
    "RACK_ENV" = "production";
    "RAILS_ENV" = "production";
    "RAILS_MAX_THREADS" = "10";
    "WEB_CONCURRENCY" = "1";
  };
};
</code></pre>
<p>I was impressed beyond impressed. I just easily saved myself hours of work... just making a timeline of events would make me want to mentally jump off a bridge.</p>
<h2>The Bigger Point</h2>
<p>What I'm trying to say, if I'm saying anything at all, is that these agents are great at so many things beyond just coding. Having the repo with the NixOS configuration just amplified the success and understanding of the problem.</p>
<p>Even better, the AI provided concrete fixes with example code:</p>
<p><img src="/images/articles/dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less/forking-around-postmortem01.png" alt="Postmortem Example"></p>
<p>Like anything else, this takes common sense... just because I let the agent run everything as root does not mean you should ðŸ˜‚... you can apply the same techniques when working with things like AWS.</p>
<p>We all know how insanely long AWS CLI commands can be... this is another area where using an LLM just makes more sense because it can run through my entire AWS account faster than I can properly craft my first CLI command to do something trivial like listing CloudWatch shit ðŸ˜‚.</p>
<h2>Tools I've Been Enjoying</h2>
<p>When doing this kind of stuff, I've found Windsurf is pretty solid, but I have also enjoyed Goose. This fucker can be both insanely useful and dangerous... you have been warned lol.</p>
<p>The real power of these tools isn't just in the code they generate, but in how they can help us make sense of complex systems. For someone with my flavor of ADHD, having an AI that can quickly parse through logs, configs, and error messages is like finally having the missing manual for my brain.</p>
<hr>
<p><em>If you're experimenting with AI for infrastructure work, start with something low-risk. Don't be like me and immediately give an AI agent root access to production systems. And when you inevitably ignore this advice, remember: good backups are your friend.</em></p>
</div></div></article></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"article":{"authors":[{"name":"James Brink","github":"https://github.com/jamesbrink"}],"content":"\u003ch1\u003eForking Around: Dangerous AI\u003c/h1\u003e\n\u003ch2\u003eHow I Use LLMs to Make Infrastructure Work Suck Less\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eBy James Brink, Tinkerer of Terror\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWelcome to another installment of \"Forking Around\" - where I document my chaotic workflows as I stumble through the brave new world of AI. I'm shocked I actually wrote a second post, defying all expectations.\u003c/p\u003e\n\u003ch2\u003eBeyond Vibe Coding: AI for the Infrastructure Crowd\u003c/h2\u003e\n\u003cp\u003eI've been seeing a flood of blogs and videos lately about AI coding, specifically this new \"vibe coding\" trend. But I'm not seeing much about infrastructure, sysadmin, or operations use cases being shared. So I figured I'd show just one of many ways I dangerously use AI to make my work much faster.\u003c/p\u003e\n\u003ch2\u003eThe Great Ubuntu â†’ NixOS Migration Adventure\u003c/h2\u003e\n\u003cp\u003eI recently migrated a production server from Ubuntu to NixOS. I had done the original server setup well over a year ago, and since I'm an infra-as-code fanatic, I naturally used Ansible. Over time I slowly learned about NixOS and became more comfortable with it in production. When we needed to take the server offline to do some storage-related work, I decided this was a prime opportunity to ditch my miles of messy Ansible and sneak in NixOS.\u003c/p\u003e\n\u003cp\u003eI branched, tested locally for a few days with a VM verifying everything was in order, and after running final backups on the prod box, I used my shiny new NixOS configuration and nixos-anywhere to install NixOS remotely over SSH, completely replacing Ubuntu. This worked flawlessly.\u003c/p\u003e\n\u003cp\u003eSo I killed a few birds with one stone and was very happy... things were working great, then one day our app randomly became unavailable. Simple Docker app behind an Nginx proxy... I quickly restarted it and all was well, but I introduced a new problem, and this really is not shocking with such a big migration. At the time of the incident, I did not have time to dig into why it happened, but I did have a gut feeling about the issue. For more context, the container was still live, no errors, the Nginx proxy was running but reporting timeouts... same old story.\u003c/p\u003e\n\u003ch2\u003eDebugging with ADHD Brain: The Old Way vs. The AI Way\u003c/h2\u003e\n\u003cp\u003eWhen I finally had time a few days later to dig into the issue, I had to mentally prepare myself for digging through all the logs, trying to keep the time offsets correct in my ADHD brain (I will never be able to do simple math in my head ðŸ˜‚).\u003c/p\u003e\n\u003cp\u003eI said fuck this... you know what... opened up my NixOS repo for this server in Windsurf, from here I just did the following:\u003c/p\u003e\n\u003cp\u003eYo, read our project structure, specifically around Podman, and Nginx services and the specific Podman service/container that failed.\u003c/p\u003e\n\u003cp\u003eThen I scrolled back through the Slack channel to find the incident... copy-pasta'd that into the prompt, with a note specifying my time zone vs. the server's so it's aware of the offsets.\u003c/p\u003e\n\u003cp\u003eI told the agent DO NOT CHANGE ANYTHING, and told it to ssh user@server -c  to run through all the logs and investigate the issue for me and report back its findings.\u003c/p\u003e\n\u003ch2\u003eThe Forensic Magic\u003c/h2\u003e\n\u003cp\u003eIt found the log entries, and when I followed up with the fact the server was recently migrated and gave it the original docker-compose, it was able to confirm with confidence that the issue was truly just cgroup related. I did not explicitly set up Podman with enough resources, and this was further confirmed by the non-error messages about Puma's current memory usage.\u003c/p\u003e\n\u003cp\u003eHere's a snippet of the timeline that the AI put together (because it's a cold-blooded machine that doesn't feel my ADHD pain):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e16:54:56 EDT: First \"upstream timed out\" error in nginx logs\u003c/li\u003e\n\u003cli\u003e16:56:03 EDT: Second \"upstream timed out\" error ( records API)\u003c/li\u003e\n\u003cli\u003e16:57:26 EDT: Third \"upstream timed out\" error\u003c/li\u003e\n\u003cli\u003e17:37:00 EDT: Issues first reported in chat\u003c/li\u003e\n\u003cli\u003e17:41:21 - 17:42:56 EDT: PumaWorkerKiller consistently reporting high memory usage (656.6MB)\u003c/li\u003e\n\u003cli\u003e17:48:24 EDT: Systemd automatically restarted the service\u003c/li\u003e\n\u003cli\u003e17:49:39 EDT: Service fully restored\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLook at that clean timeline! Do you have any idea how long it would have taken my scattered brain to piece that together manually? And it did the boring work of correlating the exact failure patterns across multiple log sources.\u003c/p\u003e\n\u003cp\u003eNow none of the above is terribly groundbreakingâ€”any sysadmin could hunt these kinds of issues down. The real difference is the absolute speed at which I was able to find the root cause. This only took a few minutes at most.\u003c/p\u003e\n\u003cp\u003eOnce I was satisfied and in agreement with the agent, I asked it to create a postmortem report for me and include the timeline of events. What it came back with was incredibly comprehensive, including a detailed root cause analysis that identified these issues:\u003c/p\u003e\n\u003ch2\u003eRoot Cause Analysis\u003c/h2\u003e\n\u003cp\u003eThe root cause of the service disruption was a combination of factors related to the recent migration from Ubuntu/Docker to NixOS/Podman:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eResource Management Differences:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Rails application was developed and tested in a Docker environment without explicit resource limits\u003c/li\u003e\n\u003cli\u003eWhen migrated to Podman on NixOS, the application encountered different resource management behavior due to cgroup v2\u003c/li\u003e\n\u003cli\u003ePumaWorkerKiller logs showed consistent high memory usage (657MB) before the failure\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eContainer Runtime Differences:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe migration from Docker to Podman introduced subtle differences in how containers are managed\u003c/li\u003e\n\u003cli\u003eThe cgroup configuration in NixOS wasn't properly set up to accommodate the container's resource needs\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIt even compared the original Docker config to my new NixOS setup:\u003c/p\u003e\n\u003ch2\u003eConfiguration Comparison\u003c/h2\u003e\n\u003cp\u003eDocker Configuration:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eweb:\n  image: \u0026#x3C;REDACTED\u003e\n  restart: always\n  network_mode: host\n  environment:\n    - VIRTUAL_PROTO=http\n    - VIRTUAL_PORT=20420\n    - VIRTUAL_HOST=\u0026#x3C;REDACTED\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNixOS/Podman Configuration:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eweb = {\n  image = \"\u0026#x3C;REDACTED\u003e\";\n  autoStart = true;\n  extraOptions = [ \"--network=host\" ];\n  environment = {\n    \"RACK_ENV\" = \"production\";\n    \"RAILS_ENV\" = \"production\";\n    \"RAILS_MAX_THREADS\" = \"10\";\n    \"WEB_CONCURRENCY\" = \"1\";\n  };\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI was impressed beyond impressed. I just easily saved myself hours of work... just making a timeline of events would make me want to mentally jump off a bridge.\u003c/p\u003e\n\u003ch2\u003eThe Bigger Point\u003c/h2\u003e\n\u003cp\u003eWhat I'm trying to say, if I'm saying anything at all, is that these agents are great at so many things beyond just coding. Having the repo with the NixOS configuration just amplified the success and understanding of the problem.\u003c/p\u003e\n\u003cp\u003eEven better, the AI provided concrete fixes with example code:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/articles/dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less/forking-around-postmortem01.png\" alt=\"Postmortem Example\"\u003e\u003c/p\u003e\n\u003cp\u003eLike anything else, this takes common sense... just because I let the agent run everything as root does not mean you should ðŸ˜‚... you can apply the same techniques when working with things like AWS.\u003c/p\u003e\n\u003cp\u003eWe all know how insanely long AWS CLI commands can be... this is another area where using an LLM just makes more sense because it can run through my entire AWS account faster than I can properly craft my first CLI command to do something trivial like listing CloudWatch shit ðŸ˜‚.\u003c/p\u003e\n\u003ch2\u003eTools I've Been Enjoying\u003c/h2\u003e\n\u003cp\u003eWhen doing this kind of stuff, I've found Windsurf is pretty solid, but I have also enjoyed Goose. This fucker can be both insanely useful and dangerous... you have been warned lol.\u003c/p\u003e\n\u003cp\u003eThe real power of these tools isn't just in the code they generate, but in how they can help us make sense of complex systems. For someone with my flavor of ADHD, having an AI that can quickly parse through logs, configs, and error messages is like finally having the missing manual for my brain.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cem\u003eIf you're experimenting with AI for infrastructure work, start with something low-risk. Don't be like me and immediately give an AI agent root access to production systems. And when you inevitably ignore this advice, remember: good backups are your friend.\u003c/em\u003e\u003c/p\u003e\n","date":"2025-04-01","excerpt":"A practical look at how AI can transform infrastructure management, debug complex issues, and save your ADHD brain from the tedium of log analysis.","slug":"dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less","title":"Forking Around: Dangerous AI - How I Use LLMs to Make Infrastructure Work Suck Less"}},"__N_SSG":true},"page":"/articles/[slug]","query":{"slug":"dangerous-ai-how-i-use-llms-to-make-infrastructure-work-suck-less"},"buildId":"Hw8_6jlXD1DGirDn_uXZH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>